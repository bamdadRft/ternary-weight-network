{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-10-24T05:59:30.866550Z","iopub.execute_input":"2024-10-24T05:59:30.866949Z","iopub.status.idle":"2024-10-24T05:59:30.872625Z","shell.execute_reply.started":"2024-10-24T05:59:30.866911Z","shell.execute_reply":"2024-10-24T05:59:30.871612Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nnum_epochs = 10\nbatch_size = 128\nlearning_rate = 0.001\nlambda_kl = 0.01  # Regularization strength for KL divergence\n\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ntrain_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\ntest_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-24T05:59:30.881937Z","iopub.execute_input":"2024-10-24T05:59:30.882280Z","iopub.status.idle":"2024-10-24T05:59:32.507185Z","shell.execute_reply.started":"2024-10-24T05:59:30.882246Z","shell.execute_reply":"2024-10-24T05:59:32.506378Z"}},"outputs":[{"name":"stdout","text":"Files already downloaded and verified\nFiles already downloaded and verified\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class SimpleCNN(nn.Module):\n    def __init__(self):\n        super(SimpleCNN, self).__init__()\n        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n        self.fc2 = nn.Linear(512, 10)\n        self.pool = nn.MaxPool2d(2, 2)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 64 * 8 * 8)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-24T05:59:32.508982Z","iopub.execute_input":"2024-10-24T05:59:32.509457Z","iopub.status.idle":"2024-10-24T05:59:32.517618Z","shell.execute_reply.started":"2024-10-24T05:59:32.509411Z","shell.execute_reply":"2024-10-24T05:59:32.516591Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"def tanh_ternary(x):\n    x_tanh = torch.tanh(x)\n    x_tanh = torch.where(x_tanh > 0.666, torch.tensor(1.0, device=x.device), x_tanh)\n    x_tanh = torch.where(x_tanh < -0.666, torch.tensor(-1.0, device=x.device), x_tanh)\n    x_tanh = torch.where((x_tanh >= -0.666) & (x_tanh <= 0.666), torch.tensor(0.0, device=x.device), x_tanh)\n    return x_tanh\n\ndef ternarize_weights(param):\n    gamma = torch.mean(torch.abs(param)) + 1e-7  # Scaling factor (to avoid division by zero)\n    return tanh_ternary(param / gamma) * gamma  # Scale the ternary weights back\n\ndef kl_divergence(original, ternary):\n    original_prob = torch.softmax(original.flatten(), dim=0)\n    ternary_prob = torch.softmax(ternary.flatten(), dim=0)\n    kl_div = F.kl_div(ternary_prob.log(), original_prob, reduction='batchmean')\n    return kl_div\n\ndef forward_with_ternarized_weights(model, x):\n    original_params = []\n    with torch.no_grad():\n        for param in model.parameters():\n            if param.requires_grad:\n                original_params.append(param.clone())\n                param.data = ternarize_weights(param.data)\n    \n    output = model(x)\n    \n    # Restore original weights\n    with torch.no_grad():\n        for param, original_param in zip(model.parameters(), original_params):\n            if param.requires_grad:\n                param.data = original_param\n    \n    return output\n\ndef calculate_accuracy(model, loader, ternary=False):\n    model.eval()\n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for images, labels in loader:\n            images, labels = images.to(device), labels.to(device)\n            if ternary:\n                outputs = forward_with_ternarized_weights(model, images)\n            else:\n                outputs = model(images)\n            _, predicted = torch.max(outputs, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    return 100 * correct / total\n\ndef train_model(ternary=False):\n    model = SimpleCNN().to(device)\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n    # Train the model for 10 epochs\n    for epoch in range(num_epochs):\n        running_loss = 0.0\n        for images, labels in tqdm(train_loader):\n            images, labels = images.to(device), labels.to(device)\n\n            # Forward pass\n            if ternary:\n                outputs = forward_with_ternarized_weights(model, images)\n            else:\n                outputs = model(images)\n\n            loss = criterion(outputs, labels)\n\n            if ternary:\n                # KL Divergence Regularization\n                kl_loss = 0.0\n                for param in model.parameters():\n                    if param.requires_grad:\n                        original_weights = param.clone().detach()\n                        ternary_weights = ternarize_weights(original_weights)\n                        kl_loss += kl_divergence(original_weights, ternary_weights)\n                \n                loss += lambda_kl * kl_loss\n\n            # Backward pass and optimization\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            running_loss += loss.item()\n\n        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n\n    return model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-24T05:59:32.519107Z","iopub.execute_input":"2024-10-24T05:59:32.519620Z","iopub.status.idle":"2024-10-24T05:59:32.538315Z","shell.execute_reply.started":"2024-10-24T05:59:32.519576Z","shell.execute_reply":"2024-10-24T05:59:32.537396Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"# Train the first model with regular weights\nprint(\"Training the first model (regular weights)...\")\nmodel_regular = train_model(ternary=False)\naccuracy_regular = calculate_accuracy(model_regular, test_loader)\nprint(f'Accuracy of the model with regular weights: {accuracy_regular:.2f}%')\n\n# Train the second model with ternarized weights in the forward pass\nprint(\"\\nTraining the second model (ternarized weights)...\")\nmodel_ternary = train_model(ternary=True)\naccuracy_ternary = calculate_accuracy(model_ternary, test_loader, ternary=True)\nprint(f'Accuracy of the model with ternarized weights: {accuracy_ternary:.2f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-10-24T05:59:32.540343Z","iopub.execute_input":"2024-10-24T05:59:32.541105Z","iopub.status.idle":"2024-10-24T06:04:39.003380Z","shell.execute_reply.started":"2024-10-24T05:59:32.541061Z","shell.execute_reply":"2024-10-24T06:04:39.002415Z"}},"outputs":[{"name":"stdout","text":"Training the first model (regular weights)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.44it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 1.3499\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 0.9445\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:13<00:00, 30.02it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 0.7801\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.54it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 0.6446\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.08it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 0.5181\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:13<00:00, 30.07it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 0.3983\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:13<00:00, 29.96it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 0.2764\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.21it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 0.1905\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:12<00:00, 30.48it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 0.1145\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:13<00:00, 29.98it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 0.0754\nAccuracy of the model with regular weights: 72.96%\n\nTraining the second model (ternarized weights)...\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.60it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [1/10], Loss: 1.5867\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.84it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [2/10], Loss: 1.2574\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.74it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [3/10], Loss: 1.1050\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.94it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [4/10], Loss: 0.9927\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.58it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [5/10], Loss: 0.9028\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.92it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [6/10], Loss: 0.8320\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.59it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [7/10], Loss: 0.7678\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.83it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [8/10], Loss: 0.6998\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.71it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [9/10], Loss: 0.6326\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 391/391 [00:17<00:00, 22.63it/s]\n","output_type":"stream"},{"name":"stdout","text":"Epoch [10/10], Loss: 0.5719\nAccuracy of the model with ternarized weights: 70.65%\n","output_type":"stream"}],"execution_count":15}]}